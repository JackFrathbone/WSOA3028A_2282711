<html lang="en">


<head>
    <title>
        Blog 12
    </title>
    <base href="../" target="_self">
    <style>
        p {
            text-align: justify;
        }
    </style>
    <script src="./js/display.js" type="text/javascript" defer></script>
    <link rel="stylesheet" type="text/css" href="./JAHS.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:title" content="JAHS OOC Blog 12" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://jackfrathbone.github.io/WSOA3028A_2282711/Blog-12.html" />
    <meta property="og:image" content="https://jackfrathbone.github.io/WSOA3028A_2282711/Images/Logo.jpeg" />
</head>

<main>

    <body>
        <article>
            <h1>
                Blog 12: <em>A Close Reading of Bergstrom and West’s Case Study — Criminal Machine Learning</em>
            </h1>
            <p>
                This reading is one that deals with the ethics of AI and algorithms, looking at how the use of machine
                learning, and other AI techniques, used in the identification of crime and criminality have ethical and
                moral implications (Bergstrom & West, 2019). It starts by introducing the case of two Chinese
                researchers who uploaded an article on using machine learning in order to identify the faces of
                criminals. This was done by looking at the faces of people and detecting what the authors referred to as
                features that are associated with criminality. The essay links this to the historic work of Cesare
                Lombroso who in the 19th century studied the idea of criminality having distinguishing physical features
                in people, but the author's state was scientifically unsound and based on both racist and classist
                assumptions about criminals and the ‘face of crime’.
            </p>
            <p>
                The crux of the essay and the most important element to take away is the next section. While the
                researchers who developed the idea of AI based ‘criminal spotting’ said that AI holds no prejudices or
                inherent bias, Bergstrom and West firmly disagree with that. They argue that while the AI may not have a
                bias in itself, it has to be trained off of data sets, which not only are selected by people such as the
                researchers but also come from real world data on crime. The two datasets used, for non-criminal and
                criminal faces also came from vastly different sources. For the criminals, ID photos were used compared
                to professional headshots which made up a large portion of the non-criminals photos. There already is a
                difference between a mandatory ID photo and one taken as a personal promotion of a person.
            </p>
            <p>
                The latter part of the article concludes that the non-criminal vs criminal identification comes from the
                fact that ID photos of criminals usually don't have a smile, while the headshots of the non-criminals
                usually have a faint smile. This tricks the AI into seeing facial differences between the two groups
                that aren’t actually facial features but expressions. This article really highlights the dangers of
                ignoring how existing bias in data and information can create a bias in AI, as well as how AI can find
                patterns that the authors might not take into account, and create their own false positives out of that.
            </p>
            <p>
                This entire article raises a bigger question about the data we use for AI, and how we source it. It is
                well known that many police forces around the world have racial, class and other biases that lead to the
                targeting of certain peoples over others, and that not every criminal gets arrested either. So even the
                most straightforward data about arrest statistics could hold a strong bias from the police and society,
                that then gets reflected in the information fed to AI.
            </p>
            <h3>References:</h3>
            <p>
                Bergstrom, C. and West, J. (2019). Case Study — Criminal Machine Learning. The University of Washington.
            </p>
        </article>
    </body>
</main>

</html>