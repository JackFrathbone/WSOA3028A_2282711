<html lang="en">


<head>
    <title>
        Blog 14
    </title>
    <base href="../" target="_self">
    <style>
        p {
            text-align: justify;
        }
    </style>
    <script src="./js/display.js" type="text/javascript" defer></script>
    <link rel="stylesheet" type="text/css" href="./JAHS.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:title" content="JAHS OOC Blog 14" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://jackfrathbone.github.io/WSOA3028A_2282711/Blog-14.html" />
    <meta property="og:image" content="https://jackfrathbone.github.io/WSOA3028A_2282711/Images/Logo.jpeg" />
</head>

<main>

    <body>
        <article>
            <h1>
                Blog 14: <em>A Critical Reflection on Crime & Machine Learning</em>
            </h1>
            <p>
                As has been discussed previously, the internet and digital technologies in general, are not neutral.
                There are biases, and flaws in every system, and no digital system is ever going to be completely free
                from the real world geographical and social divides between people. In an article from early 2020 in the
                Guardian, Henley and Booth report on the rulings of a Dutch court that found that a digital welfare
                surveillance system violated human rights (Henley & Booth, 2022). This system was based on secret
                algorithms that would collect data on certain people from all different spheres of government, such as
                their tax and income, medical history etc, and then put all these stats together to create an index of
                their likelihood of committing welfare fraud. The main ruling against this software was the complete
                secrecy it worked with, meaning people could be sanctioned by the government and lose welfare, without
                it being clear exactly how this decision was made.
            </p>
            <p>
                This algorithm based surveillance system, like all others, suffers from the issues raised in ‘Criminal
                Machine Learning’, that being the impossibility of removing biases from the input data in such a system
                (Bergstrom & West, 2019). For a welfare surveillance system to work, its algorithm needs some sort of
                training or basis for determining if someone is more likely to commit fraud, and its targets will
                generally be working class people in lower income areas. Looking at sets of data of people who have
                committed welfare fraud brings up the possibility of unintentional patterns being found, ones that could
                unfairly target people because of their medical history, tax payment or even physical looks, without
                deep oversight of the system its possible for all sorts of wrong pattern to emerge based on flaws or
                issues in the original input.
            </p>
            <p>
                Lutz, writing in the article Digital inequalities in the age of artificial intelligence and big data
                discusses the issue that arises from digital mediation of society, where private algorithms take charge
                of various facets of our life (Lutz, 2019). These range from historical inequalities becoming encoded in
                algorithms used to run whole aspects of society, like the possibility of historical racism in the
                Netherlands leading to the targeting certain ethnic groups over others in their welfare surveillance
                program, and the lack of control and say that citizens have in how these algorithms run. While they
                don’t necessarily need to be private, most government algorithms tend to be hidden, removing citizens
                from understanding and evaluating the effect these digital services can have on them, raising a whole
                host of ethical and human rights issues such as the case in the Netherlands. As more algorithms and
                machine learning based programs become commonplace in all aspects of governance, the right of citizens
                to know and decide on how their lives are run by digital mediators becomes a key issue for the near
                future.
            </p>
            <h3>References:</h3>
            <p>
                Bergstrom, C. and West, J. (2019). Case Study — Criminal Machine Learning. The University of Washington.
                <br>
                <br>
                Henley, J. and Booth, R. (2020). Welfare surveillance system violates human rights, Dutch court rules.
                [online] the Guardian. Available at: https:
                //www.theguardian.com/technology/2020/feb/05/welfare-surveillance-system-violates-human-rights-dutch-court-rules
                [Accessed 5 July 2021].
                <br>
                <br>
                Lutz, C. (2019). Digital inequalities in the age of artificial intelligence and big data. Human
                Behavior and Emerging Technologies, 1(2), pp.141-148.
            </p>
        </article>
    </body>
</main>

</html>